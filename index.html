<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reinforcement Learning &amp; MDPs Mind Map</title>
  <!-- MathJax for LaTeX formatting -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
      background-color: #f8f9fa;
    }
    .container {
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    .mindmap {
      position: relative;
      width: 1400px;
      height: 1200px;
      margin: 20px 0;
      background-color: white;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      overflow: hidden;
    }
    .node {
      position: absolute;
      padding: 12px;
      border-radius: 8px;
      cursor: pointer;
      text-align: center;
      transition: transform 0.3s ease;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      font-weight: bold;
    }
    .node:hover {
      transform: scale(1.05);
      box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    /* Colors */
    .red {
      background-color: #ffcccc;
      border: 2px solid #e60000;
      color: #990000;
    }
    .blue {
      background-color: #cce5ff;
      border: 2px solid #0066cc;
      color: #004080;
    }
    .green {
      background-color: #ccffcc;
      border: 2px solid #00cc00;
      color: #006600;
      font-weight: normal;
    }
    /* Info panel styles */
    #infoPanel {
      width: 1400px;
      min-height: 150px;
      padding: 15px;
      margin-top: 20px;
      background-color: white;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      display: none;
    }
    .infoTitle {
      font-size: 1.2em;
      font-weight: bold;
      margin-bottom: 10px;
      color: #333;
      border-bottom: 2px solid #ddd;
      padding-bottom: 5px;
    }
    .infoContent {
      line-height: 1.5;
    }
    line {
      stroke: #999;
      stroke-width: 2;
    }
    /* Legend */
    .legend {
      margin-top: 20px;
      display: flex;
      gap: 20px;
    }
    .legendItem {
      display: flex;
      align-items: center;
      gap: 8px;
    }
    .legendBox {
      width: 20px;
      height: 20px;
      border-radius: 4px;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Reinforcement Learning &amp; MDPs</h1>
    
    <div class="mindmap" id="mindmap">
      <!-- SVG for connecting lines -->
      <svg width="1400" height="1200" style="position: absolute; top: 0; left: 0;">
        <!-- Lines from central red node (center ~ (700,600)) to Blue Nodes -->
        <line x1="700" y1="600" x2="100" y2="100" />     <!-- Blue Node 1: What is RL? -->
        <line x1="700" y1="600" x2="1300" y2="100" />    <!-- Blue Node 2: Markov Decision Processes -->
        <line x1="700" y1="600" x2="1300" y2="600" />    <!-- Blue Node 3: Optimal Policies & Value Functions -->
        <line x1="700" y1="600" x2="1300" y2="1100" />   <!-- Blue Node 4: Algorithms for Solving MDPs -->
        <line x1="700" y1="600" x2="100" y2="1100" />    <!-- Blue Node 5: Summary & Challenges -->
        
        <!-- Blue Node 1 to Green Nodes (What is RL?) -->
        <line x1="100" y1="100" x2="70" y2="170" />       <!-- RL Definition -->
        <line x1="100" y1="100" x2="70" y2="230" />       <!-- Agent & Environment -->
        <line x1="100" y1="100" x2="70" y2="290" />       <!-- Applications -->
        
        <!-- Blue Node 2 to Green Nodes (MDPs) -->
        <line x1="1300" y1="100" x2="1270" y2="170" />     <!-- States & Transitions -->
        <line x1="1300" y1="100" x2="1270" y2="230" />     <!-- Reward Function -->
        <line x1="1300" y1="100" x2="1270" y2="290" />     <!-- Probabilistic Outcomes -->
        
        <!-- Blue Node 3 to Green Nodes (Optimal Policies) -->
        <line x1="1300" y1="600" x2="1270" y2="670" />     <!-- Discounted Reward: \( G = \sum_{i=0}^\infty \gamma^i R(s_i) \) -->
        <line x1="1300" y1="600" x2="1270" y2="730" />     <!-- Utility: \( U^\pi(s) = \mathbb{E}\left[ \sum_{i=0}^\infty \gamma^i R(s_i) \right] \) -->
        <line x1="1300" y1="600" x2="1270" y2="790" />     <!-- Bellman Equation: \( U(s) = R(s) + \gamma \max_a \sum_{s'} T(s'|s,a) U(s') \) -->
        
        <!-- Blue Node 4 to Green Nodes (Algorithms) -->
        <line x1="1300" y1="1100" x2="1270" y2="1170" />   <!-- Value Iteration: \( U_{\text{new}}(s) = R(s) + \gamma \max_a \sum_{s'} T(s'|s,a) U(s') \) -->
        <line x1="1300" y1="1100" x2="1270" y2="1230" />   <!-- Policy Iteration: Evaluate \( U^\pi(s) \) and update policy -->
        <line x1="1300" y1="1100" x2="1270" y2="1290" />   <!-- \( \epsilon \)-optimality: \( ||U_{\pi_{\text{valiter}}} - U^*|| \le \frac{2\epsilon\gamma}{1-\gamma} \) -->
        
        <!-- Blue Node 5 to Green Nodes (Summary & Challenges) -->
        <line x1="100" y1="1100" x2="70" y2="1170" />     <!-- Optimality vs. Approximation -->
        <line x1="100" y1="1100" x2="70" y2="1230" />     <!-- Infinite Horizon & Discounting -->
        <line x1="100" y1="1100" x2="70" y2="1290" />     <!-- Scalability & Complexity -->
      </svg>
      
      <!-- Central Red Node: Reinforcement Learning I -->
      <div class="node red" style="width: 280px; top: 560px; left: 610px;" 
           onclick="showInfo('Reinforcement Learning I', 
           '&lt;ul&gt;&lt;li&gt;RL is a paradigm where an agent learns a policy to maximize cumulative rewards by interacting with an environment.&lt;/li&gt;&lt;li&gt;The agent observes states, takes actions, and receives rewards.&lt;/li&gt;&lt;/ul&gt;')">
        Reinforcement Learning I
      </div>
      
      <!-- Blue Node 1: What is RL? -->
      <div class="node blue" style="width: 250px; top: 20px; left: 50px;" 
           onclick="showInfo('What is RL?', 
           '&lt;ul&gt;&lt;li&gt;RL involves an agent, environment, actions, states, and rewards.&lt;/li&gt;&lt;li&gt;Example: In Pacman, the agent (Pacman) navigates a maze to collect rewards.&lt;/li&gt;&lt;/ul&gt;')">
        What is RL?
      </div>
      <!-- Green Nodes for What is RL? -->
      <div class="node green" style="width: 220px; top: 100px; left: 50px;" 
           onclick="showInfo('RL Definition', 
           '&lt;ul&gt;&lt;li&gt;RL is the process of learning a policy \\(\\pi(a|s)\\) to maximize the expected return \\( G = \\sum_{i=0}^{\\infty}\\gamma^i R(s_i) \\).&lt;/li&gt;&lt;/ul&gt;')">
        RL Definition
      </div>
      <div class="node green" style="width: 220px; top: 160px; left: 50px;" 
           onclick="showInfo('Agent &amp; Environment', 
           '&lt;ul&gt;&lt;li&gt;Agent takes action \\(a_t\\) in state \\(s_t\\), then receives reward \\(r_{t+1}\\) and moves to state \\(s_{t+1}\\).&lt;/li&gt;&lt;/ul&gt;')">
        Agent &amp; Environment
      </div>
      <div class="node green" style="width: 220px; top: 220px; left: 50px;" 
           onclick="showInfo('Applications', 
           '&lt;ul&gt;&lt;li&gt;Applications include robotics, game playing, and finance.&lt;/li&gt;&lt;/ul&gt;')">
        Applications
      </div>
      
      <!-- Blue Node 2: Markov Decision Processes (MDPs) -->
      <div class="node blue" style="width: 250px; top: 20px; left: 1100px;" 
           onclick="showInfo('Markov Decision Processes', 
           '&lt;ul&gt;&lt;li&gt;MDPs model the environment: states \\(s\\), actions \\(a\\), transition probabilities \\(T(s\'|s,a)\\), and rewards \\(R(s)\\).&lt;/li&gt;&lt;li&gt;The next state is drawn from a distribution: \\( P(s\'|s,a) \\).&lt;/li&gt;&lt;/ul&gt;')">
        Markov Decision Processes
      </div>
      <!-- Green Nodes for MDPs -->
      <div class="node green" style="width: 220px; top: 100px; left: 1100px;" 
           onclick="showInfo('States &amp; Transitions', 
           '&lt;ul&gt;&lt;li&gt;State space \\(S\\) and transition relation \\(T(s\'|s,a)\\) define the dynamics.&lt;/li&gt;&lt;/ul&gt;')">
        States &amp; Transitions
      </div>
      <div class="node green" style="width: 220px; top: 160px; left: 1100px;" 
           onclick="showInfo('Reward Function', 
           '&lt;ul&gt;&lt;li&gt;Reward function \\(R(s)\\) assigns a numerical value to each state.&lt;/li&gt;&lt;/ul&gt;')">
        Reward Function
      </div>
      <div class="node green" style="width: 220px; top: 220px; left: 1100px;" 
           onclick="showInfo('Probabilistic Outcomes', 
           '&lt;ul&gt;&lt;li&gt;Actions have probabilistic outcomes (e.g., 80% intended, 10% lateral error).&lt;/li&gt;&lt;/ul&gt;')">
        Probabilistic Outcomes
      </div>
      
      <!-- Blue Node 3: Optimal Policies & Value Functions -->
      <div class="node blue" style="width: 250px; top: 500px; left: 1100px;" 
           onclick="showInfo('Optimal Policies & Value Functions', 
           '&lt;ul&gt;&lt;li&gt;The return is discounted: \\( G = \\sum_{i=0}^{\\infty}\\gamma^i R(s_i) \\).&lt;/li&gt;&lt;li&gt;Value function: \\( U^{\\pi}(s)=\\mathbb{E}\\left[ \\sum_{i=0}^{\\infty}\\gamma^i R(s_i) \\mid s_0=s,\\pi \\right] \\).&lt;/li&gt;&lt;li&gt;Bellman Equation: \\( U(s)=R(s)+\\gamma\\max_a\\sum_{s'}T(s\'|s,a)U(s\') \\).&lt;/li&gt;&lt;/ul&gt;')">
        Optimal Policies & Value Functions
      </div>
      <!-- Green Nodes for Optimal Policies -->
      <div class="node green" style="width: 220px; top: 570px; left: 1100px;" 
           onclick="showInfo('Discounted Reward', 
           '&lt;ul&gt;&lt;li&gt;Return: \\( G = R(s_0)+\\gamma R(s_1)+\\gamma^2 R(s_2)+\\cdots \\), with \\( 0\\leq\\gamma<1 \\).&lt;/li&gt;&lt;/ul&gt;')">
        Discounted Reward
      </div>
      <div class="node green" style="width: 220px; top: 630px; left: 1100px;" 
           onclick="showInfo('Utility of a State', 
           '&lt;ul&gt;&lt;li&gt;Value under policy \\( \\pi \\): \\( U^{\\pi}(s)=\\mathbb{E}[\\sum_{i=0}^\\infty \\gamma^i R(s_i)] \\).&lt;/li&gt;&lt;/ul&gt;')">
        Utility of a State
      </div>
      <div class="node green" style="width: 220px; top: 690px; left: 1100px;" 
           onclick="showInfo('Bellman Equation', 
           '&lt;ul&gt;&lt;li&gt;Recursive relation: \\( U(s)=R(s)+\\gamma\\max_a\\sum_{s'}T(s\'|s,a)U(s\') \\).&lt;/li&gt;&lt;/ul&gt;')">
        Bellman Equation
      </div>
      
      <!-- Blue Node 4: Algorithms for Solving MDPs -->
      <div class="node blue" style="width: 250px; top: 900px; left: 1100px;" 
           onclick="showInfo('Algorithms for Solving MDPs', 
           '&lt;ul&gt;&lt;li&gt;Value Iteration: Update \\( U_{new}(s)=R(s)+\\gamma\\max_a\\sum_{s'}T(s\'|s,a)U(s\') \\) iteratively until convergence.&lt;/li&gt;&lt;li&gt;Policy Iteration: Alternates between policy evaluation and policy improvement.&lt;/li&gt;&lt;/ul&gt;')">
        Algorithms for Solving MDPs
      </div>
      <!-- Green Nodes for Algorithms -->
      <div class="node green" style="width: 220px; top: 970px; left: 1100px;" 
           onclick="showInfo('Value Iteration', 
           '&lt;ul&gt;&lt;li&gt;Update rule: \\( U_{new}(s)=R(s)+\\gamma\\max_a\\sum_{s'}T(s\'|s,a)U(s\') \\).&lt;/li&gt;&lt;/ul&gt;')">
        Value Iteration
      </div>
      <div class="node green" style="width: 220px; top: 1030px; left: 1100px;" 
           onclick="showInfo('Policy Iteration', 
           '&lt;ul&gt;&lt;li&gt;Iterate between evaluating the current policy and improving it by choosing better actions.&lt;/li&gt;&lt;/ul&gt;')">
        Policy Iteration
      </div>
      <div class="node green" style="width: 220px; top: 1090px; left: 1100px;" 
           onclick="showInfo('\\(\\epsilon\\)-Optimality', 
           '&lt;ul&gt;&lt;li&gt;Error bound: \\( ||U_{\\pi_{valiter}}-U^*|| \\le \\frac{2\\epsilon\\gamma}{1-\\gamma} \\).&lt;/li&gt;&lt;/ul&gt;')">
        \\(\\epsilon\\)-Optimality
      </div>
      
      <!-- Blue Node 5: Summary & Challenges -->
      <div class="node blue" style="width: 250px; top: 900px; left: 50px;" 
           onclick="showInfo('Summary &amp; Challenges', 
           '&lt;ul&gt;&lt;li&gt;Optimal policies maximize the expected discounted return. &lt;/li&gt;&lt;li&gt;Infinite horizon problems are made tractable via discounting. &lt;/li&gt;&lt;li&gt;Scalability and approximation errors remain key challenges.&lt;/li&gt;&lt;/ul&gt;')">
        Summary &amp; Challenges
      </div>
      <!-- Green Nodes for Summary -->
      <div class="node green" style="width: 220px; top: 970px; left: 50px;" 
           onclick="showInfo('Optimality vs. Approximation', 
           '&lt;ul&gt;&lt;li&gt;Optimal policy: \\( \\pi^*(s)=\\arg\\max_a \\sum_{s'}T(s\'|s,a)U(s\') \\).&lt;/li&gt;&lt;/ul&gt;')">
        Optimality vs. Approximation
      </div>
      <div class="node green" style="width: 220px; top: 1030px; left: 50px;" 
           onclick="showInfo('Infinite Horizon & Discounting', 
           '&lt;ul&gt;&lt;li&gt;Discount factor \\( \\gamma \\) (with \\( 0\\leq\\gamma<1 \\)) ensures convergence.&lt;/li&gt;&lt;/ul&gt;')">
        Infinite Horizon & Discounting
      </div>
      <div class="node green" style="width: 220px; top: 1090px; left: 50px;" 
           onclick="showInfo('Scalability & Complexity', 
           '&lt;ul&gt;&lt;li&gt;Large state spaces necessitate approximate methods.&lt;/li&gt;&lt;/ul&gt;')">
        Scalability & Complexity
      </div>
    </div>
    
    <!-- Information Panel -->
    <div id="infoPanel">
      <div class="infoTitle" id="infoTitle">Click on a concept to see details</div>
      <div class="infoContent" id="infoContent">
        Select any node in the mind map to display detailed information.
      </div>
    </div>
    
    <!-- Legend -->
    <div class="legend">
      <div class="legendItem">
        <div class="legendBox red"></div>
        <span>Big Picture Concepts</span>
      </div>
      <div class="legendItem">
        <div class="legendBox blue"></div>
        <span>Major Categories</span>
      </div>
      <div class="legendItem">
        <div class="legendBox green"></div>
        <span>Details &amp; Equations</span>
      </div>
    </div>
  </div>
  
  <script>
    function showInfo(title, content) {
      document.getElementById("infoPanel").style.display = "block";
      document.getElementById("infoTitle").textContent = title;
      document.getElementById("infoContent").innerHTML = content;
      MathJax.typesetPromise();
    }
  </script>
</body>
</html>
